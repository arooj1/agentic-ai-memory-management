{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Implementing self-editing memory from scratch\n",
    "\n",
    "\n",
    "<div style=\"background-color:grey; padding:13px; border-width:3px; border-color:#efe6ef; border-style:solid; border-radius:6px\">\n",
    "There are a variety of ways to enable long-term memory in LLM agents, such as RAG and recursive summarization. The MemGPT paper first introduced the notion of *self-editing memory*. Essentially, offload memory management to the LLM. After all, the LLM is the most \"intelligent\" part of our programs, so why not have the LLM figure out memory instead of hard coding some solution?\n",
    "\n",
    "In this section, we'll walk through how to use OpenAI's tool calling to implement some simple memory management tools.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Gemini API configured successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gl_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Install the Google Generative AI library\n",
    "#!pip install -q google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "import os\n",
    "import sys\n",
    "import uuid\n",
    "import json\n",
    "import time\n",
    "from IPython.display import display, Markdown\n",
    "# Import the function from your helper file\n",
    "from helper import get_gemini_api_key\n",
    "\n",
    "def print_markdown(text):\n",
    "    \"\"\"Prints text as markdown in a notebook.\"\"\"\n",
    "    display(Markdown(text))\n",
    "\n",
    "# --- Configuration ---\n",
    "try:\n",
    "    api_key = get_gemini_api_key()\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key is missing. Please ensure your helper.py file returns a valid key.\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"\\nGemini API configured successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred during configuration: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Core Agent and Tool Logic\n",
    "This section contains the reusable GeminiAgent class, the tool definitions and implementations, and the main run_agent_step function that handles the interaction loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Agent Class ---\n",
    "class GeminiAgent:\n",
    "    \"\"\"A class to simulate an agent's state and memory.\"\"\"\n",
    "    def __init__(self, name, model_name, system_prompt=\"\", memory_blocks=None, tools=None):\n",
    "        self.id = f\"agent-{uuid.uuid4()}\"\n",
    "        self.name = name\n",
    "        self.memory_blocks = memory_blocks if memory_blocks else {}\n",
    "        self.tools = tools if tools else []\n",
    "        self.system_prompt = system_prompt\n",
    "        self.model = genai.GenerativeModel(\n",
    "            model_name=model_name,\n",
    "            tools=self.tools\n",
    "        )\n",
    "\n",
    "    def get_formatted_memory(self):\n",
    "        \"\"\"Formats memory blocks into a string for the system prompt.\"\"\"\n",
    "        if not self.memory_blocks:\n",
    "            return \"\"\n",
    "        formatted_string = \"--- CORE MEMORY ---\\n\"\n",
    "        for label, value in self.memory_blocks.items():\n",
    "            val_str = json.dumps(value) if isinstance(value, list) else str(value)\n",
    "            formatted_string += f\"<{label}>\\n{val_str}\\n</{label}>\\n\"\n",
    "        return formatted_string.strip()\n",
    "\n",
    "# --- Reusable Printing Function ---\n",
    "def print_message(message_type, content):\n",
    "    \"\"\"Prints formatted messages based on their type.\"\"\"\n",
    "    if message_type == \"reasoning\":\n",
    "        print(f\"ðŸ§  Reasoning: {content}\")\n",
    "    elif message_type == \"assistant\":\n",
    "        print(f\"ðŸ¤– Agent: {content}\")\n",
    "    elif message_type == \"tool_call\":\n",
    "        tool_name = content.get(\"name\", \"N/A\")\n",
    "        arguments = content.get(\"arguments\", {})\n",
    "        print(f\"ðŸ”§ Tool Call: {tool_name}\\n{json.dumps(arguments, indent=2)}\")\n",
    "    elif message_type == \"tool_return\":\n",
    "        print(f\"ðŸ”§ Tool Return: {content}\")\n",
    "    elif message_type == \"user\":\n",
    "        print(f\"ðŸ‘¤ User Message: {content}\")\n",
    "    else:\n",
    "        print(content)\n",
    "    print(\"-----------------------------------------------------\")\n",
    "\n",
    "# --- Tool Definitions (for the model) ---\n",
    "def core_memory_save(section: str, memory: str):\n",
    "    \"\"\"\n",
    "    Save important information about you, the agent, or the human you are chatting with.\n",
    "    Args:\n",
    "        section (str): Must be either 'human' (to save information about the human) or 'agent' (to save information about yourself).\n",
    "        memory (str): Memory to save in the section.\n",
    "    \"\"\"\n",
    "    pass\n",
    "\n",
    "# --- Tool Implementations (for Python) ---\n",
    "def _core_memory_save_impl(agent: \"GeminiAgent\", section: str, memory: str):\n",
    "    \"\"\"Implementation for saving to core memory.\"\"\"\n",
    "    if section in agent.memory_blocks:\n",
    "        agent.memory_blocks[section] += f\"\\n{memory}\"\n",
    "    else:\n",
    "        agent.memory_blocks[section] = memory\n",
    "    return f\"Memory saved to section '{section}'. Current memory: {json.dumps(agent.memory_blocks)}\"\n",
    "\n",
    "# --- Generic Agent Interaction Loop ---\n",
    "def run_agent_step(agent, tool_registry, user_message, chat_history=None):\n",
    "    \"\"\"Handles a multi-step agent interaction.\"\"\"\n",
    "    if chat_history is None:\n",
    "        chat_history = []\n",
    "\n",
    "    print_message(\"user\", user_message)\n",
    "    \n",
    "    system_prompt = (\n",
    "        f\"{agent.system_prompt}\\n\"\n",
    "        \"You must either call a tool (core_memory_save) or write a response to the user. \"\n",
    "        \"Do not take the same actions multiple times! \"\n",
    "        \"When you learn new information, make sure to always call the core_memory_save tool.\"\n",
    "    )\n",
    "    \n",
    "    full_prompt = f\"{system_prompt}\\n\\n{agent.get_formatted_memory()}\\n\\n**Task:**\\n{user_message}\"\n",
    "    \n",
    "    messages = chat_history + [{'role': 'user', 'parts': [{'text': full_prompt}]}]\n",
    "\n",
    "    while True:\n",
    "        response = agent.model.generate_content(messages, tools=agent.tools)\n",
    "        time.sleep(1)\n",
    "        \n",
    "        if not response.candidates:\n",
    "            final_text = \"No response was generated. This might be due to safety settings.\"\n",
    "            break\n",
    "            \n",
    "        message = response.candidates[0].content\n",
    "        messages.append(message)\n",
    "\n",
    "        if not any(part.function_call for part in message.parts):\n",
    "            final_text = message.parts[0].text if message.parts else \"Task complete.\"\n",
    "            break\n",
    "\n",
    "        tool_response_parts = []\n",
    "        for part in message.parts:\n",
    "            if not part.function_call: continue\n",
    "            fc = part.function_call\n",
    "            tool_name = fc.name\n",
    "            tool_args = dict(fc.args)\n",
    "            print_message(\"reasoning\", f\"Model wants to call `{tool_name}`.\")\n",
    "            print_message(\"tool_call\", {\"name\": tool_name, \"arguments\": tool_args})\n",
    "            \n",
    "            # Pass the agent instance to the implementation\n",
    "            result = tool_registry[tool_name](agent=agent, **tool_args)\n",
    "            \n",
    "            print_message(\"tool_return\", result)\n",
    "            tool_response_parts.append({\"function_response\": {\"name\": tool_name, \"response\": {\"content\": result}}})\n",
    "        \n",
    "        messages.append({\"role\": \"tool\", \"parts\": tool_response_parts})\n",
    "\n",
    "    print_message(\"assistant\", final_text)\n",
    "    return final_text, messages\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Section 1: Implementing Editable Memory\n",
    "This section demonstrates the core concept of self-editing memory by creating an agent that can save information to its own memory blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¤– Agent: I do not know your name.  I have no memory of past conversations and no access to personal information about you unless you explicitly provide it.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ¤– Agent: Your name is Bob.\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ‘¤ User Message: my name is bob.\n",
      "-----------------------------------------------------\n",
      "ðŸ§  Reasoning: Model wants to call `core_memory_save`.\n",
      "-----------------------------------------------------\n",
      "ðŸ”§ Tool Call: core_memory_save\n",
      "{\n",
      "  \"memory\": \"My name is bob.\",\n",
      "  \"section\": \"human\"\n",
      "}\n",
      "-----------------------------------------------------\n",
      "ðŸ”§ Tool Return: Memory saved to section 'human'. Current memory: {\"human\": \"\\nMy name is bob.\", \"agent\": \"\"}\n",
      "-----------------------------------------------------\n",
      "ðŸ¤– Agent: OK. I'll remember that your name is Bob.\n",
      "\n",
      "-----------------------------------------------------\n",
      "\n",
      "Updated Agent Memory:\n",
      "{\n",
      "  \"human\": \"\\nMy name is bob.\",\n",
      "  \"agent\": \"\"\n",
      "}\n",
      "ðŸ‘¤ User Message: what is my name\n",
      "-----------------------------------------------------\n",
      "ðŸ¤– Agent: Your name is Bob.\n",
      "\n",
      "-----------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- 1. A simple agent's context window ---\n",
    "model = \"gemini-1.5-flash\"\n",
    "system_prompt = \"You are a chatbot.\"\n",
    "chat_completion = genai.GenerativeModel(model).generate_content(\n",
    "    [\n",
    "        {'role': 'user', 'parts': [{'text': system_prompt}]},\n",
    "        {'role': 'user', 'parts': [{'text': \"What is my name?\"}]}\n",
    "    ]\n",
    ")\n",
    "print(f\"ðŸ¤– Agent: {chat_completion.text}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 2. Adding memory to the context ---\n",
    "agent_memory = {\"human\": \"Name: Bob\"}\n",
    "system_prompt_with_memory = (\n",
    "    \"You are a chatbot. \"\n",
    "    \"You have a section of your context called [MEMORY] \"\n",
    "    \"that contains information relevant to your conversation.\"\n",
    ")\n",
    "full_prompt = f\"{system_prompt_with_memory}\\n\\n[MEMORY]\\n{json.dumps(agent_memory)}\"\n",
    "chat_completion = genai.GenerativeModel(model).generate_content(\n",
    "    [\n",
    "        {'role': 'user', 'parts': [{'text': full_prompt}]},\n",
    "        {'role': 'user', 'parts': [{'text': \"What is my name?\"}]}\n",
    "    ]\n",
    ")\n",
    "print(f\"ðŸ¤– Agent: {chat_completion.text}\")\n",
    "print(\"=\"*50 + \"\\n\")\n",
    "\n",
    "# --- 3. Modifying memory with tools ---\n",
    "editable_memory_agent = GeminiAgent(\n",
    "    name=\"editable_memory_agent\",\n",
    "    model_name=\"gemini-1.5-flash\",\n",
    "    system_prompt=system_prompt_with_memory,\n",
    "    memory_blocks={\"human\": \"\", \"agent\": \"\"},\n",
    "    tools=[core_memory_save]\n",
    ")\n",
    "\n",
    "tool_registry = {\n",
    "    \"core_memory_save\": _core_memory_save_impl\n",
    "}\n",
    "\n",
    "# --- 4. Implementing an agentic loop ---\n",
    "final_response, chat_history = run_agent_step(\n",
    "    agent=editable_memory_agent,\n",
    "    tool_registry=tool_registry,\n",
    "    user_message=\"my name is bob.\"\n",
    ")\n",
    "\n",
    "# --- 5. Verifying the memory update ---\n",
    "print(\"\\nUpdated Agent Memory:\")\n",
    "print(json.dumps(editable_memory_agent.memory_blocks, indent=2))\n",
    "\n",
    "# --- 6. Running the next agent step ---\n",
    "final_response, chat_history = run_agent_step(\n",
    "    agent=editable_memory_agent,\n",
    "    tool_registry=tool_registry,\n",
    "    user_message=\"what is my name\",\n",
    "    chat_history=chat_history\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
