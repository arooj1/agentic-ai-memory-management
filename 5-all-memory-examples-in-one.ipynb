{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Gemini API configured successfully!\n",
      "--- Main Context (RAM) ---\n",
      "{\n",
      "  \"system_prompt\": \"You are a helpful assistant.\",\n",
      "  \"working_memory\": {\n",
      "    \"user_name\": \"Alex\"\n",
      "  },\n",
      "  \"chat_history\": [\n",
      "    \"User: Hi!\",\n",
      "    \"Agent: Hello Alex!\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "--- External Context (Disk) ---\n",
      "{\n",
      "  \"document_id_123\": \"The company's vacation policy allows for 20 days off per year...\",\n",
      "  \"document_id_456\": \"The Q3 financial report shows a 10% increase in revenue...\"\n",
      "}\n",
      "Initial memory: {'human': ''}\n",
      "‚úÖ Memory updated in section: 'human'\n",
      "Final memory: {'human': \"\\nThe human's name is Bob.\"}\n",
      "üîé Searching external memory for: 'vacation policies'\n",
      "üìÑ Retrieved Context: 'The company's vacation policy allows for 20 days off per year...'\n",
      "\n",
      "--- Final Prompt for Generation ---\n",
      "\n",
      "Context:\n",
      "The company's vacation policy allows for 20 days off per year...\n",
      "\n",
      "Question:\n",
      "What is the company's vacation policy?\n",
      "\n",
      "Answer the question based on the provided context.\n",
      "\n",
      "--- Original Document (Visible to Hiring Manager) ---\n",
      "{\n",
      "  \"Name\": \"John Doe\",\n",
      "  \"Phone\": \"555-1234\",\n",
      "  \"Address\": \"123 Main St\",\n",
      "  \"Skills\": \"Python, SQL\"\n",
      "}\n",
      "\n",
      "--- Redacted Document (Visible to Junior Analyst) ---\n",
      "{\n",
      "  \"Name\": \"[REDACTED]\",\n",
      "  \"Phone\": \"[REDACTED]\",\n",
      "  \"Address\": \"[REDACTED]\",\n",
      "  \"Skills\": \"Python, SQL\"\n",
      "}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/gl_env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "# ü§ñ From LLMs to Operating Systems: The MemGPT Approach\n",
    "\n",
    "Inspired by the [MemGPT paper](https://arxiv.org/pdf/2310.08560) and the [DeepLearning.AI course](https://learn.deeplearning.ai/courses/llms-as-operating-systems-agent-memory), this notebook explores how to overcome the memory limitations of LLMs by treating them like the CPU of a traditional operating system.\n",
    "\"\"\"\n",
    "\n",
    "#@title ## 1. Setup and Configuration\n",
    "# Install the Google Generative AI library\n",
    "#!pip install -q google-generativeai\n",
    "\n",
    "import google.generativeai as genai\n",
    "import json\n",
    "# Import the function from your helper file\n",
    "from helper import get_gemini_api_key\n",
    "\n",
    "# --- Configuration ---\n",
    "# Note: To run this, you'll need a helper.py file with a get_gemini_key() function.\n",
    "try:\n",
    "    api_key = get_gemini_api_key()\n",
    "    if not api_key:\n",
    "        raise ValueError(\"API key not found in helper.py\")\n",
    "    genai.configure(api_key=api_key)\n",
    "    print(\"‚úÖ Gemini API configured successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"üö® Error configuring API key: {e}\")\n",
    "    print(\"Please ensure your helper.py file is in the same directory and returns a valid API key.\")\n",
    "\n",
    "\"\"\"---\n",
    "## Concept 1: The Memory Hierarchy üß†\n",
    "\n",
    "Just like a computer has fast RAM and slow disk storage, an agent has a tiered memory system. The key is to intelligently manage what information is loaded into the LLM's limited \"working memory\" (the context window).\n",
    "\"\"\"\n",
    "\n",
    "#@title ### Example: Main Context (RAM) vs. External Context (Disk)\n",
    "\n",
    "# The LLM's direct context window.\n",
    "# Fast, but limited in size.\n",
    "main_context = {\n",
    "    \"system_prompt\": \"You are a helpful assistant.\",\n",
    "    \"working_memory\": {\"user_name\": \"Alex\"},\n",
    "    \"chat_history\": [\"User: Hi!\", \"Agent: Hello Alex!\"]\n",
    "}\n",
    "\n",
    "# A vector database or file system where long-term knowledge is stored.\n",
    "# Vast, but slower to access.\n",
    "external_context = {\n",
    "    \"document_id_123\": \"The company's vacation policy allows for 20 days off per year...\",\n",
    "    \"document_id_456\": \"The Q3 financial report shows a 10% increase in revenue...\"\n",
    "}\n",
    "\n",
    "print(\"--- Main Context (RAM) ---\")\n",
    "print(json.dumps(main_context, indent=2))\n",
    "print(\"\\n--- External Context (Disk) ---\")\n",
    "print(json.dumps(external_context, indent=2))\n",
    "\n",
    "\"\"\"---\n",
    "## Concept 2: Self-Editing Memory ‚úçÔ∏è\n",
    "\n",
    "The core idea of MemGPT: give the LLM the tools to manage its own memory. Instead of us deciding what's important, the agent learns to save critical information itself.\n",
    "\"\"\"\n",
    "\n",
    "#@title ### Example: Saving User Information with a Tool\n",
    "\n",
    "# 1. We start with an empty memory block for the 'human'.\n",
    "agent_memory = {\"human\": \"\"}\n",
    "\n",
    "# 2. Define the tool implementation in Python.\n",
    "def core_memory_save(section: str, memory: str):\n",
    "    \"\"\"A simple function to update our agent's memory dictionary.\"\"\"\n",
    "    if section in agent_memory:\n",
    "        agent_memory[section] += f\"\\n{memory}\"\n",
    "        print(f\"‚úÖ Memory updated in section: '{section}'\")\n",
    "    else:\n",
    "        print(f\"‚ùå Error: Section '{section}' not found in memory.\")\n",
    "\n",
    "# 3. Simulate the LLM's decision to call the tool.\n",
    "# The user says \"My name is Bob.\" The LLM decides this is important.\n",
    "llm_tool_call = {\n",
    "    \"name\": \"core_memory_save\",\n",
    "    \"arguments\": {\n",
    "        \"section\": \"human\",\n",
    "        \"memory\": \"The human's name is Bob.\"\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"Initial memory:\", agent_memory)\n",
    "\n",
    "# 4. Our code executes the tool call.\n",
    "core_memory_save(**llm_tool_call[\"arguments\"])\n",
    "\n",
    "print(\"Final memory:\", agent_memory)\n",
    "\n",
    "\"\"\"---\n",
    "## Concept 3: Agentic RAG üìö\n",
    "\n",
    "This memory system enables a powerful, agent-driven RAG process. The agent actively decides when it needs more information and goes to get it from its external memory.\n",
    "\"\"\"\n",
    "\n",
    "#@title ### Example: Answering a Question with RAG\n",
    "\n",
    "# 1. The user asks a question the agent can't answer from its working memory.\n",
    "user_question = \"What is the company's vacation policy?\"\n",
    "\n",
    "# 2. Simulate the LLM's decision to search external memory.\n",
    "llm_tool_call_rag = {\n",
    "    \"name\": \"archival_memory_search\",\n",
    "    \"arguments\": {\n",
    "        \"query\": \"vacation policies\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# 3. Define the RAG tool implementation.\n",
    "def archival_memory_search(query: str):\n",
    "    \"\"\"Simulates searching our external context (vector DB).\"\"\"\n",
    "    print(f\"üîé Searching external memory for: '{query}'\")\n",
    "    if \"vacation\" in query:\n",
    "        return external_context[\"document_id_123\"]\n",
    "    return \"No relevant information found.\"\n",
    "\n",
    "# 4. Our code executes the tool call.\n",
    "retrieved_context = archival_memory_search(**llm_tool_call_rag[\"arguments\"])\n",
    "print(f\"üìÑ Retrieved Context: '{retrieved_context}'\")\n",
    "\n",
    "# 5. The retrieved context is \"augmented\" into the next prompt,\n",
    "# allowing the LLM to generate a fact-based answer.\n",
    "final_prompt = f\"\"\"\n",
    "Context:\n",
    "{retrieved_context}\n",
    "\n",
    "Question:\n",
    "{user_question}\n",
    "\n",
    "Answer the question based on the provided context.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n--- Final Prompt for Generation ---\")\n",
    "print(final_prompt)\n",
    "\n",
    "\"\"\"---\n",
    "## Concept 4: Memory Redaction in Multi-Agent Systems üîí\n",
    "\n",
    "When multiple agents share memory, we need a way to control access to sensitive information. An \"Admin\" agent with special privileges can redact confidential data before sharing it with other agents.\n",
    "\"\"\"\n",
    "\n",
    "#@title ### Example: Redacting PII for a Junior Agent\n",
    "\n",
    "# 1. A shared document with sensitive information.\n",
    "shared_document = {\n",
    "    \"Name\": \"John Doe\",\n",
    "    \"Phone\": \"555-1234\",\n",
    "    \"Address\": \"123 Main St\",\n",
    "    \"Skills\": \"Python, SQL\"\n",
    "}\n",
    "\n",
    "# 2. The \"Admin Controller\" has a redaction tool.\n",
    "def redact_memory(document, fields_to_redact):\n",
    "    \"\"\"Redacts specified fields from a document.\"\"\"\n",
    "    redacted_doc = document.copy()\n",
    "    for field in fields_to_redact:\n",
    "        if field in redacted_doc:\n",
    "            redacted_doc[field] = \"[REDACTED]\"\n",
    "    return redacted_doc\n",
    "\n",
    "# 3. A Junior Analyst agent requests the document. The Admin Controller intercepts.\n",
    "fields_to_redact = [\"Name\", \"Phone\", \"Address\"]\n",
    "redacted_document = redact_memory(shared_document, fields_to_redact)\n",
    "\n",
    "print(\"--- Original Document (Visible to Hiring Manager) ---\")\n",
    "print(json.dumps(shared_document, indent=2))\n",
    "\n",
    "print(\"\\n--- Redacted Document (Visible to Junior Analyst) ---\")\n",
    "print(json.dumps(redacted_document, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gl_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
